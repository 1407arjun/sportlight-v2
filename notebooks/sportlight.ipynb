{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_DIR=\"../samples/\"\n",
    "ETC_DIR=SAMPLES_DIR+\"etc/\"\n",
    "AUDIO_DIR=SAMPLES_DIR+\"audio/\"\n",
    "OUT_DIR=SAMPLES_DIR+\"out/\"\n",
    "VIDEO_NAME=\"video.mp4\"\n",
    "AUDIO_NAME=\"video.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "video = AudioSegment.from_file(SAMPLES_DIR+IN_NAME, format=\"mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = video.set_channels(1).set_frame_rate(16000).set_sample_width(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='../samples/out/video.wav'>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.export(AUDIO_DIR+AUDIO_NAME, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/sportlight/lib/python3.9/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "output = model.transcribe(AUDIO_DIR+AUDIO_NAME)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = list()\n",
    "\n",
    "add = [\"Straight\", \"biggie\", \"Cover\", \"OnDrive\", \"Square\", \"Forward\", \"stadium\", \"Defence\", \"Sweep\", \"Reverse\",\n",
    "           \"FrontFoot \", \"LegGlance \", \"BackFoot\", \"SquareCut\", \"Pull \", \"Shot\", \"Hook\", \"Uppercut\", \"Cut\", \"Helicopter \", \"SwitchHit\",\n",
    "           \"Dilscoop\", \"class\", \"bounce\", \"Upper\", \"Uppish\", \"Scoop \", \"Inside\", \"Out\", \"Shots\", \"Bouncer\", \"Outswinger\", \"Inswinger\",\n",
    "           \"ReverseSwing\", \"played\", \"LegCutter\", \"OffCutter\", \"Yorker\", \"Slower\", \"Spin\", \"LegBreak \", \"OffBreak\", \"Googly \",\n",
    "           \"Doosra\", \"Topspin \", \"CarromBall\", \"Slider\", \"ArmBall\", \"Infield\", \"InnerRing\", \"Outfield\", \"Catching\", \"Wicketkeeper\",\n",
    "           \"Slip\", \"Gully\", \"LegSlip\", \"LegGully\", \"Sillypoint\", \"Sillymidoff\", \"Shortleg\", \"Sillymidon\", \"InnerRing\", \"Point\", \"BackwardPoint\",\n",
    "           \"MidOff\", \"Cover\", \"MidOn\", \"SquareLeg\", \"Backward \", \"SquareLeg\", \"MidWicket\", \"FineLeg\", \"Outfield\", \"ThirdMan\",\n",
    "           \"DeepPoint\", \"BackwardPoint\", \"ExtraCover\", \"LongOff\", \"FineLeg\", \"LongLeg\", \"LongOn\", \"Deep\", \"Cover\", \"played\", \"account\"\n",
    "           \"cricket\", \"hard\", \"sides\", \"man\", \"finishes\", \"one\", \"crucial\", \"Captain\", \"shot\", \"six\", \"four\", \"boundary\", \"line\", \"drive\",\n",
    "           \"celebrate\", \"placement\", \"beauty\", \"fifty\", \"century\", \"perfect\", \"magnifcient\", \"world\", \"cup\", \"batting\", \"fielding\", \"bowling\",\n",
    "           \"catch\", \"caught\", \"out\", \"stumped\", \"one\", \"bowled\", \"night\", \"final\", \"room\", \"taken\", \"edged\", \"wicket\", \"review\", \"DRS\", \"cuts\", \"out\", \"short\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/arjunsivaraman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/arjunsivaraman/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "for i in add:\n",
    "        for synset in wordnet.synsets(i):\n",
    "            for lemma in synset.lemmas():\n",
    "                positive.append(lemma.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ' '.join(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = output['segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "start_time = []\n",
    "end_time = []\n",
    "\n",
    "for i in data:\n",
    "    text.append(i['text'])\n",
    "    start_time.append(i['start'])\n",
    "    end_time.append(i['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_dataframe(matrix, tokens):\n",
    "    doc_names = [f'doc_{i+1}' for i, _ in enumerate(matrix)]\n",
    "    df = pd.DataFrame(data=matrix, index=doc_names, columns=tokens)\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "score = []\n",
    "for i in text:\n",
    "    d = [i, strings]\n",
    "    Tfidf_vect = TfidfVectorizer()\n",
    "    vector_matrix = Tfidf_vect.fit_transform(d)\n",
    "    tokens = Tfidf_vect.get_feature_names_out()\n",
    "    create_dataframe(vector_matrix.toarray(), tokens)\n",
    "    cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
    "    result = create_dataframe(\n",
    "        cosine_similarity_matrix, ['Phrase', 'Strings'])\n",
    "    score.append(result['Phrase'].values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_first = []\n",
    "stack_last = []\n",
    "\n",
    "for i in range(len(score)):\n",
    "    if (score[i] >= 0.00500000000000):\n",
    "        stack_first.append(start_time[i])\n",
    "        stack_last.append(end_time[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort both lists based on one list\n",
    "# stack_first, stack_last = zip(*sorted(zip(stack_first, stack_last)))\n",
    "result = sorted(zip(stack_first, stack_last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 8.0),\n",
       " (37.0, 45.0),\n",
       " (45.0, 49.0),\n",
       " (49.0, 51.0),\n",
       " (51.0, 55.0),\n",
       " (61.0, 64.0),\n",
       " (73.0, 81.0),\n",
       " (83.0, 89.0),\n",
       " (135.0, 139.0),\n",
       " (166.0, 171.0),\n",
       " (171.0, 180.0),\n",
       " (185.0, 193.0),\n",
       " (198.0, 203.0),\n",
       " (217.0, 223.0),\n",
       " (228.0, 234.0),\n",
       " (239.0, 243.0),\n",
       " (246.0, 252.0)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [[result[0][0] - 4 if result[0][0] - 4 > 0 else 0, result[0][1] + 3]]\n",
    "for i in range(1, len(result)):\n",
    "    if res[-1][1] + 3 >= result[i][0] - 4:\n",
    "        if res[-1][1] + 3 < result[i][1] + 3:\n",
    "            res[-1][1] = result[i][1] + 3\n",
    "    else:\n",
    "        res.append([result[i][0] - 4 if result[i][0] - 4 > 0 else 0, result[i][1] + 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 11.0], [33.0, 92.0], [131.0, 142.0], [162.0, 206.0], [213.0, 255.0]]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
